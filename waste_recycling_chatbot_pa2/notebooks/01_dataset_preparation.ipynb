{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b255a5cb",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Preprocessing for Waste Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3c439",
   "metadata": {},
   "source": [
    "This notebook prepares the Helene waste dataset for machine learning model training. The process includes loading labeled waste images, creating standardized category mappings, and organizing the data into train/validation/test splits while maintaining class distribution. The resulting structured dataset enables efficient training and evaluation of the waste classification chatbot system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f87096",
   "metadata": {},
   "source": [
    "### 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44007353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1244c2",
   "metadata": {},
   "source": [
    "### 2. Configuration Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d15075",
   "metadata": {},
   "source": [
    "This code adds the project's config directory to the Python path and imports the dataset file paths from the configuration file. It sets up three main path variables: SOURCE (original dataset), OUTPUT (processed data destination), and excel_path (waste category labels file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89091561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source dataset exists: True\n",
      "Labels file exists: True\n"
     ]
    }
   ],
   "source": [
    "# Add config directory to Python path for importing project settings\n",
    "project_root = Path(r\"C:\\Users\\Lejlum\\Documents\\PA2_Recycling_Chatbot\\waste_recycling_chatbot_pa2\\config\")\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import dataset paths and configuration from config file\n",
    "from config import WASTE_CHATBOT_DATASET, PROCESSED_NEW, WASTE_CHATBOT_EXCEL\n",
    "\n",
    "# Set source dataset and output directories\n",
    "SOURCE = WASTE_CHATBOT_DATASET          # Original dataset location\n",
    "OUTPUT = PROCESSED_NEW                  # Processed dataset output location  \n",
    "excel_path = WASTE_CHATBOT_EXCEL        # Excel file with waste category labels\n",
    "\n",
    "print(f\"Source dataset exists: {SOURCE.exists()}\")\n",
    "print(f\"Labels file exists: {excel_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f12ce2",
   "metadata": {},
   "source": [
    "### 3. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798829fb",
   "metadata": {},
   "source": [
    "The Excel file containing waste image labels and metadata is loaded into a pandas DataFrame. Column names are defined for processing - \"ID\" for image filenames and \"Label\" for waste categories. Basic dataset information including shape, column names, and sample rows are displayed to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863f1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5795, 4)\n",
      "Columns: ['ID', 'Label', 'Source', 'Prompt Cumo V1']\n",
      "\n",
      "First few rows:\n",
      "         ID              Label    Source  \\\n",
      "0  0001.jpg  Plastic,Aluminium  own work   \n",
      "1  0002.jpg  Plastic,Aluminium  own work   \n",
      "2  0003.jpg  Plastic,Aluminium  own work   \n",
      "3  0004.jpg  Plastic,Aluminium  own work   \n",
      "4  0005.jpg  Plastic,Aluminium  own work   \n",
      "\n",
      "                                      Prompt Cumo V1  \n",
      "0  This item consists of plastic and aluminium. T...  \n",
      "1  This item consists of plastic and aluminium. T...  \n",
      "2  This item consists of plastic and aluminium. T...  \n",
      "3  This item consists of plastic and aluminium. T...  \n",
      "4  This item consists of plastic and aluminium. T...  \n"
     ]
    }
   ],
   "source": [
    "# Load Excel file containing image labels and metadata\n",
    "df = pd.read_excel(excel_path) \n",
    "\n",
    "# Define column names for processing\n",
    "id_col = \"ID\"           # Column containing image filenames/IDs\n",
    "label_col = \"Label\"     # Column containing waste category labels\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a38dd2",
   "metadata": {},
   "source": [
    "### 4. Label Processing and Category Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade00b64",
   "metadata": {},
   "source": [
    "Unique waste categories are extracted from the dataset and sorted alphabetically. A mapping dictionary is created to convert original label names into standardized folder names by converting to lowercase and replacing spaces and commas with underscores. The output displays all detected categories with their standardized versions and shows the total number of waste categories found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931acfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected waste categories:\n",
      "Aluminium -> aluminium\n",
      "Brown Glass -> brown_glass\n",
      "Cardboard -> cardboard\n",
      "Composite Carton -> composite_carton\n",
      "Green Glass -> green_glass\n",
      "Hazardous waste (Battery) -> hazardous_waste_(battery)\n",
      "Metal -> metal\n",
      "Organic waste -> organic_waste\n",
      "PET -> pet\n",
      "Paper -> paper\n",
      "Plastic -> plastic\n",
      "Plastic,Aluminium -> plastic_aluminium\n",
      "Residual waste -> residual_waste\n",
      "Rigid plastic container -> rigid_plastic_container\n",
      "White Glass -> white_glass\n",
      "White Glass,Metal -> white_glass_metal\n",
      "\n",
      "Total number of categories: 16\n"
     ]
    }
   ],
   "source": [
    "# Extract unique waste categories from dataset\n",
    "labels = sorted(df[label_col].astype(str).str.strip().unique().tolist())\n",
    "\n",
    "# Create mapping from original labels to standardized folder names\n",
    "# Convert to lowercase and replace spaces/commas with underscores\n",
    "MAPPING = {label: label.lower().replace(\" \", \"_\").replace(\",\", \"_\") for label in labels}\n",
    "\n",
    "print(\"Detected waste categories:\")\n",
    "for original, standardized in MAPPING.items():\n",
    "    print(f\"{original} -> {standardized}\")\n",
    "\n",
    "print(f\"\\nTotal number of categories: {len(MAPPING)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11932faf",
   "metadata": {},
   "source": [
    "### 5. Directory Structure Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d67df0",
   "metadata": {},
   "source": [
    "A nested folder structure is created for the dataset organization. Three main directories (train, val, test) are generated, each containing subdirectories for every waste category. The `mkdir()` function with `parents=True` creates all necessary parent directories, while `exist_ok=True` prevents errors if directories already exist. The final structure follows the pattern: OUTPUT/split/category/ for organized model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca28d206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory structure:\n",
      "Base output directory: C:\\Users\\Lejlum\\Documents\\PA2_Recycling_Chatbot\\data\\processed_new\\organized_dataset\n",
      "Splits: ['train', 'val', 'test']\n",
      "Categories per split: 16\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation/test split directories\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Create folder structure: OUTPUT/split/category/\n",
    "for split in splits:\n",
    "    for category in MAPPING.values():\n",
    "        (OUTPUT / split / category).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Created directory structure:\")\n",
    "print(f\"Base output directory: {OUTPUT}\")\n",
    "print(f\"Splits: {splits}\")\n",
    "print(f\"Categories per split: {len(MAPPING)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dfc55",
   "metadata": {},
   "source": [
    "### 6. Data Splitting Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f1126",
   "metadata": {},
   "source": [
    "A stratified split is performed to maintain proportional class distribution across all subsets. The dataset is divided into 70% training, 15% validation, and 15% test data through two sequential splits. The `stratify` parameter ensures each waste category appears proportionally in all splits, while `random_state=42` guarantees reproducible results. Split statistics are displayed showing the number and percentage of images in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad76029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split summary:\n",
      "train: 4056 images (70.0%)\n",
      "val: 869 images (15.0%)\n",
      "test: 870 images (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified split to maintain class distribution\n",
    "# First split: 70% train, 30% temp (for val+test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.3, \n",
    "    stratify=df[label_col], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 15% val, 15% test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    stratify=temp_df[label_col], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Store splits in dictionary for easy iteration\n",
    "splits_data = {\n",
    "    \"train\": train_df,\n",
    "    \"val\": val_df,\n",
    "    \"test\": test_df\n",
    "}\n",
    "\n",
    "# Display split statistics\n",
    "print(\"Dataset split summary:\")\n",
    "for split_name, split_df in splits_data.items():\n",
    "    print(f\"{split_name}: {len(split_df)} images ({len(split_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081924e9",
   "metadata": {},
   "source": [
    "### 7. File Copy Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e12f01",
   "metadata": {},
   "source": [
    "Images are copied from the source directory to their designated split and category folders. For each dataset split, the process iterates through all assigned images with a progress bar display. Source image paths are constructed using the ID column, and missing files are skipped with a warning message. Target paths follow the structure OUTPUT/split/category/image_name, with shutil.copy2() preserving file metadata during the copy operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b603d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying train images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 4056/4056 [00:53<00:00, 75.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying val images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|██████████| 869/869 [00:11<00:00, 77.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 870/870 [00:11<00:00, 75.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Copy images to appropriate directories based on split and category\n",
    "for split_name, split_df in splits_data.items():\n",
    "    print(f\"\\nCopying {split_name} images...\")\n",
    "    \n",
    "    for _, row in tqdm(split_df.iterrows(), total=len(split_df), desc=f\"Processing {split_name}\"):\n",
    "        # Source image path\n",
    "        src = SOURCE / row[id_col]\n",
    "        \n",
    "        # Skip if source image doesn't exist\n",
    "        if not src.exists():\n",
    "            print(f\"Warning: Image {src.name} not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Determine target category folder\n",
    "        category_folder = MAPPING[row[label_col]]\n",
    "        \n",
    "        # Target path: OUTPUT/split/category/image_name\n",
    "        dst = OUTPUT / split_name / category_folder / src.name\n",
    "        \n",
    "        # Copy image with metadata preservation\n",
    "        shutil.copy2(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96233a9",
   "metadata": {},
   "source": [
    "### 8. Summary and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e7dbd",
   "metadata": {},
   "source": [
    "Image counts are verified for each dataset split by traversing the created directory structure and counting files in all category subdirectories. The validation process ensures all images were successfully copied to their designated locations. Final statistics display the total number of images per split and confirm the processed dataset location for subsequent model training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56961312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 4056 images\n",
      "VAL: 869 images\n",
      "TEST: 870 images\n",
      "\n",
      "Processed dataset location: C:\\Users\\Lejlum\\Documents\\PA2_Recycling_Chatbot\\data\\processed_new\\organized_dataset\n"
     ]
    }
   ],
   "source": [
    "# Count images in each split for validation\n",
    "for split_name in splits_data.keys():\n",
    "    split_path = OUTPUT / split_name\n",
    "    total_images = sum(len(list(category_path.glob(\"*\"))) \n",
    "                      for category_path in split_path.iterdir() \n",
    "                      if category_path.is_dir())\n",
    "    print(f\"{split_name.upper()}: {total_images} images\")\n",
    "\n",
    "print(f\"\\nProcessed dataset location: {OUTPUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa_leo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
